{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "a = np.random.randn(1000000)\n",
    "b = np.random.randn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.1922883361947\n",
      "Time taken in vectorized form is :0.9934902191162109 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check with vectorized form\n",
    "start= time.time()\n",
    "c = np.dot(a,b)\n",
    "end = time.time()\n",
    "print(c)\n",
    "print(\"Time taken in vectorized form is :{} seconds\".format((end-start)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.1922883361761\n",
      "Time taken in non-vectorized form is :337.77689933776855 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now check non vetorized form\n",
    "start = time.time()\n",
    "c = 0\n",
    "for i in range(1000000):\n",
    "    c += a[i] * b[i]\n",
    "end = time.time()\n",
    "print(c)\n",
    "print(\"Time taken in non-vectorized form is :{} seconds\".format((end-start)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    1.2   1.8]\n",
      " [  0.  104.  135. ]\n",
      " [  4.4  52.   99. ]\n",
      " [ 68.    8.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Find avg percentages of carb,fat protien \n",
    "\n",
    "A = np.array([[56.0,1.2,1.8],\n",
    "              [0.0,104.0,135.0],\n",
    "              [4.4,52.0,99.0],\n",
    "              [68.0,8.0,0.9]]\n",
    "              )\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59. , 239. , 155.4,  76.9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 100*(A/cal.reshape(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.91525424,  0.        ,  2.83140283, 88.42652796],\n",
       "       [ 2.03389831, 43.51464435, 33.46203346, 10.40312094],\n",
       "       [ 3.05084746, 56.48535565, 63.70656371,  1.17035111]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 3)\n",
    "b = np.random.randn(3, 1)\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 6, 8],\n",
       "       [2, 4, 6, 8],\n",
       "       [2, 4, 6, 8]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims = True)\n",
    "    print(x_sum)\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8260.88614278]\n",
      " [1248.04631753]]\n",
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 64, 64, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "train_set_x_flatten =  train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_set_x_flatten =  test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 209)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    #expo = np.exp(z)\n",
    "    #expo_sum = np.sum(np.exp(z))\n",
    "    #return expo/expo_sum\n",
    "    #return np.maximum(0,z)\n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    w = np.zeros((dim,1),dtype=float)\n",
    "    b = 0.0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0.0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)                                    # compute activation\n",
    "    cost = -1/m*(np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)))               # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    dw = (1/m) * (np.dot(X,(A-Y).T))\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (â‰ˆ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * b\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.17998823]\n",
      " [0.07550727]]\n",
      "b = 0.8098329520228539\n",
      "dw = [[0.61299061]\n",
      " [1.31774648]]\n",
      "db = 0.0633680265420237\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "        if A[0,i] <= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 5000, learning_rate = 0.001, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (â‰ˆ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (â‰ˆ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.591308\n",
      "Cost after iteration 200: 0.555820\n",
      "Cost after iteration 300: 0.529005\n",
      "Cost after iteration 400: 0.506912\n",
      "Cost after iteration 500: 0.487913\n",
      "Cost after iteration 600: 0.471142\n",
      "Cost after iteration 700: 0.456080\n",
      "Cost after iteration 800: 0.442385\n",
      "Cost after iteration 900: 0.429815\n",
      "Cost after iteration 1000: 0.418196\n",
      "Cost after iteration 1100: 0.407393\n",
      "Cost after iteration 1200: 0.397299\n",
      "Cost after iteration 1300: 0.387830\n",
      "Cost after iteration 1400: 0.378915\n",
      "Cost after iteration 1500: 0.370495\n",
      "Cost after iteration 1600: 0.362523\n",
      "Cost after iteration 1700: 0.354955\n",
      "Cost after iteration 1800: 0.347756\n",
      "Cost after iteration 1900: 0.340894\n",
      "Cost after iteration 2000: 0.334342\n",
      "Cost after iteration 2100: 0.328075\n",
      "Cost after iteration 2200: 0.322072\n",
      "Cost after iteration 2300: 0.316314\n",
      "Cost after iteration 2400: 0.310785\n",
      "Cost after iteration 2500: 0.305467\n",
      "Cost after iteration 2600: 0.300348\n",
      "Cost after iteration 2700: 0.295416\n",
      "Cost after iteration 2800: 0.290658\n",
      "Cost after iteration 2900: 0.286064\n",
      "Cost after iteration 3000: 0.281626\n",
      "Cost after iteration 3100: 0.277333\n",
      "Cost after iteration 3200: 0.273179\n",
      "Cost after iteration 3300: 0.269156\n",
      "Cost after iteration 3400: 0.265257\n",
      "Cost after iteration 3500: 0.261475\n",
      "Cost after iteration 3600: 0.257806\n",
      "Cost after iteration 3700: 0.254242\n",
      "Cost after iteration 3800: 0.250781\n",
      "Cost after iteration 3900: 0.247416\n",
      "Cost after iteration 4000: 0.244143\n",
      "Cost after iteration 4100: 0.240959\n",
      "Cost after iteration 4200: 0.237859\n",
      "Cost after iteration 4300: 0.234840\n",
      "Cost after iteration 4400: 0.231898\n",
      "Cost after iteration 4500: 0.229031\n",
      "Cost after iteration 4600: 0.226234\n",
      "Cost after iteration 4700: 0.223507\n",
      "Cost after iteration 4800: 0.220845\n",
      "Cost after iteration 4900: 0.218246\n",
      "Cost after iteration 5000: 0.215708\n",
      "Cost after iteration 5100: 0.213228\n",
      "Cost after iteration 5200: 0.210805\n",
      "Cost after iteration 5300: 0.208437\n",
      "Cost after iteration 5400: 0.206121\n",
      "Cost after iteration 5500: 0.203856\n",
      "Cost after iteration 5600: 0.201639\n",
      "Cost after iteration 5700: 0.199470\n",
      "Cost after iteration 5800: 0.197347\n",
      "Cost after iteration 5900: 0.195267\n",
      "train accuracy: 97.12918660287082 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 6000, learning_rate = 0.001, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "train_set_x_flatten =  train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_set_x_flatten =  test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    w = np.zeros((dim,1),dtype=float)\n",
    "    b = 0.0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)                                    # compute activation\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)))               # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    dw = (1/m) * (np.dot(X,(A-Y).T))\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (â‰ˆ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * b\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "        if A[0,i] <= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (â‰ˆ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (â‰ˆ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
